\input{../PyCharm_Beamer_Preamble.tex}
% !Tex spellceck = en_US

\title[MA Seminar Talk - Progress]{
	\centering
	\includegraphics[width=0.3\textwidth]{images/WLLT}\\
	Master Thesis Seminar Talk	
}
%\title[MA Seminar Talk - Progress]{Master Thesis Seminar Talk}
%\titlegraphic{
%	\includegraphics[width=0.2\textwidth]{images/WLLT}
%}
\subtitle{Progress Update}
\author[F. Beaumont]{Fabrice Beaumont}
\institute[]{Department of Information Systems and Artificial Intelligence - \textbf{Dr. Pascal Welke}}
\date{10. August 2022}

\newcommand{\figureWidth}{7cm}
\newcommand{\figureHorizontal}{2cm}
\newcommand{\figureVertical}{5cm}
\begin{document}


\begin{frame}
	\titlepage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview}

\begin{frame}
\frametitle{Recap progress} \vspace{-1cm}
	\begin{itemize}
		\item Cleaning the datasets
		\item Preparing comparison
		\item Re-thinking the WLLT structure
		\item \textbf{Tree-Wasserstein distances}\\
		{\tiny [2019, Tam Le, Tree-Sliced Variants of Wasserstein Distances]}
		\item \textcolor{orange}{\enquote{Naive} feedback loop}
	\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

\begin{frame}
\frametitle{Naive feedback loop}
	\begin{itemize}
		\item Initialize all edge weights as $1.0$.
		\item Compute the \textit{Tree Wasserstein Distance}\footnote{Normalized weighted distance between their wl-label histograms.} between two graphs
		\item Pics the \textcolor{green}{$n$} highest differences in the weighted difference vector.\footnote{Most expensive earth that had to be moved.}
		\item Push \textcolor{green}{and} pull graphs by changing the weights py percentage (\textcolor{green}{$0.1$}):
		\[ w^\prime  =\begin{cases}
		w * (1 + \textcolor{green}{p_{\text{push}}} )\\
		w * (1 - \textcolor{green}{p_{\text{pull}}} )
		\end{cases}  \] 
		\item[] 
		\item[] 
%		\[ w^\prime  = w^\prime * \frac{\sum w}{\sum w^\prime} \]
		\item[] 
		\item[]
	\end{itemize}
\end{frame}

\begin{frame}[noframenumbering]
	\frametitle{Naive feedback loop}
	\begin{itemize}
		\item Initialize all edge weights as $1.0$.
		\item Compute the \textit{Tree Wasserstein Distance}\footnote{Normalized weighted distance between their wl-label histograms.} between two graphs
		\item Pics the \textcolor{green}{$n$} highest differences in the weighted difference vector.\footnote{Most expensive earth that had to be moved.}
		\item Push \textcolor{green}{and} pull graphs by changing the weights py percentage (\textcolor{green}{$0.1$}):
		\[ w^\prime  =\begin{cases}
		w * (1 + \textcolor{green}{p_{\text{push}}} )\\
		w * (1 - \textcolor{green}{p_{\text{pull}}} )
		\end{cases}  \] 
		\item[]
		\item[!] \textcolor{orange}{Ensure that the sum of the edge weights is the same}
		%		\[ w^\prime  = w^\prime * \frac{\sum w}{\sum w^\prime} \]
		\item[!] \textcolor{orange}{Ensure that the impact on the weights is proportional to the number of graphs in the sample}
	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Evaluation process}
	Implemented:
	\begin{itemize}
		\item Silhouette score \newline
		\item Mean weight per WLLT layer
	\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Outlook for august} \vspace{-1cm}
	\begin{enumerate}
		\item Play with the settings, document different results \newline	
		\item Fix and extend the evaluation process:
		\begin{itemize}
			\item Mean \textbf{intra distance} in and \textbf{inter distance} between clusters
			\item Percentage of changed weights
			\item \textbf{Classification accuracy} compared to other methods
		\end{itemize}
	\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\frametitle{Further outlook} \vspace{-1cm}
	\textbf{Further outlook}:
	\begin{itemize}
		\item Implement different edge weight training:
		\begin{itemize}
			\item Batch learning, Weight update after each distance computation
			\item Treat weights in WLLT layers differently (e.g. update only leaves)
			\item Update all weights in the WLLT path
			\item \dots \newline
		\end{itemize}
		\item Initialize edge weights via FRM method
	\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{End}

\begin{frame}[c]
	\centering %\Huge
	\begin{huge}
		\emph{Thank you all for listening.}\\
	\end{huge}
	\vspace{2 cm}
	I will be happy to answer any \textbf{questions} and\\
	hear your \textbf{comments}.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\section{Appendix}


\begin{frame}[noframenumbering]
	\frametitle{Preparation of the performance comparison}	
	\begin{figure}
		\centering
		\includegraphics[width=0.6\linewidth]{images/plot_whiteText}
		\caption{Classification accuracies on databases using Weisfeiler-Lehman.}
		\label{fig:plot}
	\end{figure}
	\tiny{\texttt{grakel.kernels.\textbf{WeisfeilerLehman}(n\_iter=[1-10], base=grakel.kernels.VertexHistogram, normalize=True)}}\\
	\tiny{\texttt{grakel.utils.\textbf{cross\_validate\_Kfold\_SVM}(K, y, n\_iter=10)}}
\end{frame}

\begin{frame}[noframenumbering]
	\frametitle{Example of the whole procedure}
	\begin{tikzpicture}[remember picture, overlay]
	\node[right] at (current page.west) 
	{
		\includegraphics[width=0.9\textwidth]{images/Graphs7}
	};
	\end{tikzpicture}
	\begin{columns}[T]%beamer
		\column{0.7\textwidth}
		% blank
		\column{0.3\textwidth}
		\textbf{Tree metric}:\\
		$\begin{array}{ccccc}
		\ \ \; \textit{4} & \textit{5} & \textit{6} & \textit{7} & \textit{8}
		\end{array}$\\
		$
		\left(		
		\begin{array}{ccccc}
		\cdot & 2 & 4 & 4 & 4 \\
		& \cdot & 4 & 4 & 4 \\
		&   & \cdot & 2 & 4 \\
		& \upuparrows & & \cdot & 4\\
		&   &   &   & \cdot
		\end{array}
		\right)
		$\\
		\textbf{Wasserstein Dist.}:\\
		$\mathcal{W}_{t}(A,B) = \frac{4}{3}$\\
		$\mathcal{W}_{t}(A,C) = 3$\\
		$\mathcal{W}_{t}(B,C) = 3$\\
	\end{columns}
	\vspace{0.5cm}
	$d_{\text{WLLT}}(B,C) = 2*\frac{2}{4} + 4*\frac{1}{4} + 4*\frac{1}{4} = \frac{12}{4} = 3$
\end{frame}

\begin{frame}[noframenumbering]
	\frametitle{Example of the whole procedure}	
	\begin{columns}[T]%beamer
		\column{0.5\textwidth}
		\centering
		\textbf{Current clustering:}\\
		\vspace{0.5cm}
		\includegraphics[width=0.8\textwidth]{images/Classification1}
		\column{0.5\textwidth}
		\centering
		\textbf{Target clustering:}\\
		\vspace{0.5cm}
		\includegraphics[width=0.8\textwidth]{images/Classification2}
	\end{columns}
	\vspace{0.5cm}
	Idea: Reduce distance between $B$ and $C$, by updating the edge weights.
\end{frame}

\begin{frame}[noframenumbering]
	\frametitle{Example of the whole procedure}	
	\begin{columns}[T]%beamer
		\column{0.5\textwidth}
		\centering		
		\textit{Local update} $P_{7,8}$:
		\includegraphics[width=0.8\textwidth]{images/WLLTUpdateEnds}
		\textit{Weighted path update} $P_{7,8}$:
		\includegraphics[width=0.8\textwidth]{images/WLLTUpdatePath}
		\column{0.5\textwidth}
		\begin{tikzpicture}[remember picture, overlay]
		\node[left] at (current page.east) [xshift=2.5cm, yshift=-0.5cm] 
		{
			\includegraphics[width=1.2\textwidth]{images/Graphs7}
		};
		\end{tikzpicture}
	\end{columns}
\end{frame}

\begin{frame}[noframenumbering]
\frametitle{Implementation road-map 1/2}
\begin{itemize}
	\item \textbf{WLLT Construction}:
		\begin{itemize}
			\item Write to file and read from file. Construct WL-iteration based.
			\item All weights \textit{equal}.
			\item (\textit{Random} initial weights.)
			\item (Use \textit{a priori} knowledge.)
		\end{itemize}
	\item[]
	\item \textbf{Wasserstein-Distance feedback}:
		\begin{itemize}
			\item \enquote{Biggest pile of dirt}. 
			(\enquote{Smallest}, to increase the distance.)
			\item Distribution proportional to the pile size.
			\item Distribution proportional to the cost of moving the pile size.
		\end{itemize}	
\end{itemize}	
\end{frame}

\begin{frame}[noframenumbering]
\frametitle{Implementation road-map 2/2}
\begin{itemize}
	\item \textbf{Update rule}:
	\begin{itemize}
		\item Value:
		\begin{itemize}
			\item Constant $\lambda$.
			\item \textit{Gradient descent}.
		\end{itemize}
		\item Location:
		\begin{itemize}
			\item \textit{Local}: Only update the first and last edge weights of the connecting path.
			\item \textit{Weighted path}: Update all edge weights on the path, with less magnitude for edges closer to the root.
			\item \textit{Path}: Update all edges on the path.
			\item \textit{Global}: Update all edges, related to all occurring labels.
		\end{itemize}
	\end{itemize}		
\end{itemize}	
\end{frame}

\end{document}